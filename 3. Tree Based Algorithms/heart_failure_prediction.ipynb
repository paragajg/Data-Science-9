{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "##### Scikit Learn modules needed for Logistic Regression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler , StandardScaler\n",
    "\n",
    "## Below packages are needed for Hyper Parameter Tuning of an Algorithm in Scikit Learn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes = True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"heart_failure_clinical_records_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of data: (299, 13)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of data: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                         0\n",
       "anaemia                     0\n",
       "creatinine_phosphokinase    0\n",
       "diabetes                    0\n",
       "ejection_fraction           0\n",
       "high_blood_pressure         0\n",
       "platelets                   0\n",
       "serum_creatinine            0\n",
       "serum_sodium                0\n",
       "sex                         0\n",
       "smoking                     0\n",
       "time                        0\n",
       "DEATH_EVENT                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Blanks cells\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n",
       "       'ejection_fraction', 'high_blood_pressure', 'platelets',\n",
       "       'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time',\n",
       "       'DEATH_EVENT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Inputdata : (299, 5)\n",
      "Shape of Target : (299,)\n"
     ]
    }
   ],
   "source": [
    "X = df.loc[:,['high_blood_pressure','platelets','sex','smoking','diabetes']]\n",
    "print(f\"Shape of Inputdata : {X.shape}\")\n",
    "Y = df.loc[:,\"DEATH_EVENT\"]\n",
    "print(f\"Shape of Target : {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Xs:(269, 5)\n",
      "Shape of Test Xs:(30, 5)\n",
      "Shape of Training y:(269,)\n",
      "Shape of Test y:(30,)\n"
     ]
    }
   ],
   "source": [
    "# Train & Test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.10,\n",
    "                                                    random_state=21)\n",
    "\n",
    "print('Shape of Training Xs:{}'.format(x_train.shape))\n",
    "print('Shape of Test Xs:{}'.format(x_test.shape))\n",
    "print('Shape of Training y:{}'.format(y_train.shape))\n",
    "print('Shape of Test y:{}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_features = X.columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', MinMaxScaler())])\n",
    "\n",
    "# categorical_features = \n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        #('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier(random_state= 42))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model on test data: 0.5\n"
     ]
    }
   ],
   "source": [
    "clf.fit(x_train,y_train)\n",
    "print(f\"Accuracy of model on test data: {clf.score(x_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 288 candidates, totalling 576 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   46.1s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best Model from grid search: 0.500\n",
      "Optimum setting of hyperparameters:................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 576 out of 576 | elapsed:  1.9min finished\n",
      "/Users/paragpradhan/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'gini',\n",
       " 'classifier__max_depth': 10,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__n_estimators': 50,\n",
       " 'preprocessor__num__imputer__strategy': 'mean',\n",
       " 'preprocessor__num__scaler': StandardScaler()}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()],\n",
    "    'classifier__criterion': [\"gini\",\"entropy\"],\n",
    "    'classifier__max_features': [\"auto\",\"sqrt\",\"log2\"],\n",
    "    'classifier__max_depth':[10,50,100],\n",
    "    'classifier__n_estimators':[10,50,150,200]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=2, iid=False,verbose = 1,n_jobs= -1)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "print((\"best Model from grid search: %.3f\"\n",
    "       % grid_search.score(x_test, y_test)))\n",
    "# Print your best combination of hyper parameters\n",
    "print(\"Optimum setting of hyperparameters:................\")\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
